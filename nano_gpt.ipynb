{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ftVatsjwYk-m",
        "outputId": "97f47541-3b73-4bbf-9615-ecddcf1ca6e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.5.15)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.7.4)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.7.0\n"
          ]
        }
      ],
      "source": [
        "! pip install tiktoken\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import math\n",
        "import time\n",
        "import inspect\n",
        "from dataclasses import dataclass\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import tiktoken\n",
        "import numpy as np\n",
        "from torch.utils.data import Dataset, DataLoader\n"
      ],
      "metadata": {
        "id": "kKPEE1AgYpMd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class GPTConfig:\n",
        "    block_size: int = 1024 # max sequence length\n",
        "    vocab_size: int = 50257 # number of tokens: 50,000 BPE merges + 256 bytes tokens + 1 <|endoftext|> token\n",
        "    n_layer: int = 12 # number of layers\n",
        "    n_head: int = 12 # number of heads\n",
        "    n_embd: int = 768 # embedding dimension"
      ],
      "metadata": {
        "id": "adjrjVjLYpOs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CausalSelfAttention(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        assert config.n_embd % config.n_head == 0\n",
        "        # key, query, value projections for all heads, but in a batch\n",
        "        self.c_attn = nn.Linear(config.n_embd, 3 * config.n_embd)\n",
        "        # output projection\n",
        "        self.c_proj = nn.Linear(config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "        # regularization\n",
        "        self.n_head = config.n_head\n",
        "        self.n_embd = config.n_embd\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
        "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
        "        # nh is \"number of heads\", hs is \"head size\", and C (number of channels) = nh * hs\n",
        "        # e.g. in GPT-2 (124M), n_head=12, hs=64, so nh*hs=C=768 channels in the Transformer\n",
        "        qkv = self.c_attn(x)\n",
        "        q, k, v = qkv.split(self.n_embd, dim=2)\n",
        "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
        "        y = F.scaled_dot_product_attention(q, k, v, is_causal=True) # flash attention\n",
        "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
        "        # output projection\n",
        "        y = self.c_proj(y)\n",
        "        return y\n",
        "\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd)\n",
        "        self.gelu    = nn.GELU(approximate='tanh')\n",
        "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd)\n",
        "        self.c_proj.NANOGPT_SCALE_INIT = 1\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.c_fc(x)\n",
        "        x = self.gelu(x)\n",
        "        x = self.c_proj(x)\n",
        "        return x\n",
        "\n",
        "class Block(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.ln_1 = nn.LayerNorm(config.n_embd)\n",
        "        self.attn = CausalSelfAttention(config)\n",
        "        self.ln_2 = nn.LayerNorm(config.n_embd)\n",
        "        self.mlp = MLP(config)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.ln_1(x))\n",
        "        x = x + self.mlp(self.ln_2(x))\n",
        "        return x\n",
        "\n",
        "class GPT(nn.Module):\n",
        "\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config = config\n",
        "\n",
        "        self.transformer = nn.ModuleDict(dict(\n",
        "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
        "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
        "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
        "            ln_f = nn.LayerNorm(config.n_embd),\n",
        "        ))\n",
        "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
        "\n",
        "        # weight sharing scheme\n",
        "        self.transformer.wte.weight = self.lm_head.weight\n",
        "\n",
        "        # init params\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            std = 0.02\n",
        "            if hasattr(module, 'NANOGPT_SCALE_INIT'):\n",
        "                std *= (2 * self.config.n_layer) ** -0.5\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=std)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # idx is of shape (B, T)\n",
        "        B, T = idx.size()\n",
        "        assert T <= self.config.block_size, f\"Cannot forward sequence of length {T}, block size is only {self.config.block_size}\"\n",
        "        # forward the token and posisition embeddings\n",
        "        pos = torch.arange(0,T , dtype=torch.long, device=idx.device) # shape (T)\n",
        "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (T, n_embd)\n",
        "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (B, T, n_embd)\n",
        "        x = tok_emb + pos_emb\n",
        "        # forward the blocks of the transformer\n",
        "        for block in self.transformer.h:\n",
        "            x = block(x)\n",
        "        # forward the final layernorm and the classifier\n",
        "        x = self.transformer.ln_f(x)\n",
        "        logits = self.lm_head(x) # (B, T, vocab_size)\n",
        "        # print(logits)\n",
        "        loss = None\n",
        "        if targets is not None:\n",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))\n",
        "        return logits, loss\n",
        "\n",
        "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
        "        # start with all of the candidate parameters (that require grad)\n",
        "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
        "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
        "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
        "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
        "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
        "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
        "        optim_groups = [\n",
        "            {'params': decay_params, 'weight_decay': weight_decay},\n",
        "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
        "        ]\n",
        "        num_decay_params = sum(p.numel() for p in decay_params)\n",
        "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
        "        # Create AdamW optimizer and use the fused version if it is available\n",
        "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
        "        use_fused = fused_available and device_type == \"cuda\"\n",
        "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, betas=(0.9, 0.95), eps=1e-8, fused=use_fused)\n",
        "        return optimizer"
      ],
      "metadata": {
        "id": "ms12VMCMYpRW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    device = \"cuda\"\n",
        "elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
        "    device = \"mps\"\n",
        "print(f\"using device: {device}\")\n",
        "\n",
        "# added after video, pytorch can be serious about it's device vs. device_type distinction\n",
        "device_type = \"cuda\" if device.startswith(\"cuda\") else \"cpu\"\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed(1337)\n",
        "\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "total_batch_size = 524288 # 2**19, ~0.5M, in number of tokens\n",
        "B = 64 # micro batch size\n",
        "T = 1024 # sequence length\n",
        "ddp_rank = 0\n",
        "ddp_world_size = 1\n",
        "master_process = True\n",
        "grad_accum_steps = total_batch_size // (B * T * ddp_world_size)\n",
        "torch.set_float32_matmul_precision('high')\n",
        "\n",
        "# create model\n",
        "model = GPT(GPTConfig(vocab_size=50304))\n",
        "# model = GPT.from_pretrained(\"gpt2\") # or init from OpenAI GPT-2\n",
        "model.to(device)\n",
        "use_compile = False # torch.compile interferes with HellaSwag eval and Generation. TODO fix\n",
        "if use_compile:\n",
        "    model = torch.compile(model)\n",
        "raw_model = model # always contains the \"raw\" unwrapped model\n",
        "\n",
        "max_lr = 6e-4\n",
        "min_lr = max_lr * 0.1\n",
        "warmup_steps = 715\n",
        "max_steps = 19073 # 19,073 steps is ~1 epoch, if data is 10B tokens and batch size 0.5M tokens\n",
        "def get_lr(it):\n",
        "    # 1) linear warmup for warmup_iters steps\n",
        "    if it < warmup_steps:\n",
        "        return max_lr * (it+1) / warmup_steps\n",
        "    # 2) if it > lr_decay_iters, return min learning rate\n",
        "    if it > max_steps:\n",
        "        return min_lr\n",
        "    # 3) in between, use cosine decay down to min learning rate\n",
        "    decay_ratio = (it - warmup_steps) / (max_steps - warmup_steps)\n",
        "    assert 0 <= decay_ratio <= 1\n",
        "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff starts at 1 and goes to 0\n",
        "    return min_lr + coeff * (max_lr - min_lr)\n",
        "\n",
        "# optimize!\n",
        "optimizer = raw_model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device_type)\n",
        "\n",
        "# create the log directory we will write checkpoints to and log to\n",
        "log_dir = \"log\"\n",
        "os.makedirs(log_dir, exist_ok=True)\n",
        "log_file = os.path.join(log_dir, f\"log.txt\")\n",
        "with open(log_file, \"w\") as f: # open for writing to clear the file\n",
        "    pass"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WKQ6xSamYpUA",
        "outputId": "d557bf46-bc2b-4793-83ea-98c79c53a6f3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('shahname_fa.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()\n",
        "\n",
        "tokensq = enc.encode(text)\n",
        "\n",
        "with open('tokens.txt', 'w') as file:\n",
        "    file.write(' '.join(map(str, tokensq)))\n",
        "\n",
        "print(f\"Tokenized {len(tokensq)} tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ShJ4BSFTYpWh",
        "outputId": "2e989cbd-6c8c-418e-eb58-c30a3f6b29c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tokenized 3067442 tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class ShahnamehDataset(Dataset):\n",
        "    def __init__(self, file_path, seq_length):\n",
        "        \"\"\"\n",
        "        Custom Dataset for Shahnameh token sequences.\n",
        "\n",
        "        Args:\n",
        "            file_path (str): Path to the file containing the tokenized text.\n",
        "            seq_length (int): Length of each sequence to be used for training.\n",
        "        \"\"\"\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Read the tokens from the file and convert to integers\n",
        "        with open(file_path, 'r') as file:\n",
        "            tokens = list(map(int, file.read().split()))\n",
        "\n",
        "        # Trim tokens to ensure they can be evenly divided into sequences\n",
        "        num_sequences = len(tokens) // seq_length\n",
        "        self.tokens = tokens[:num_sequences * seq_length]\n",
        "\n",
        "        # Convert tokens list to a PyTorch tensor\n",
        "        self.tokens_tensor = torch.tensor(self.tokens, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Returns the total number of sequences in the dataset.\n",
        "        \"\"\"\n",
        "        return len(self.tokens_tensor) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        \"\"\"\n",
        "        Returns a single sequence and its target.\n",
        "\n",
        "        Args:\n",
        "            idx (int): Index of the sequence.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (input sequence, target sequence)\n",
        "        \"\"\"\n",
        "        input_seq = self.tokens_tensor[idx:idx + self.seq_length]\n",
        "        target_seq = self.tokens_tensor[idx + 1:idx + self.seq_length + 1]\n",
        "        return input_seq, target_seq\n",
        "\n",
        "    def next_batch(self, batch_size):\n",
        "        \"\"\"\n",
        "        Creates a DataLoader to yield batches of sequences.\n",
        "\n",
        "        Args:\n",
        "            batch_size (int): Number of sequences per batch.\n",
        "\n",
        "        Returns:\n",
        "            DataLoader: DataLoader object to iterate over batches.\n",
        "        \"\"\"\n",
        "        return DataLoader(self, batch_size=batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "XdmxwjgElvGA"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = 'tokens.txt'\n",
        "seq_length = 1024\n",
        "\n",
        "dloader = ShahnamehDataset(file_path, seq_length)\n",
        "\n",
        "batch_size = 5\n",
        "train_loader = dloader.next_batch(batch_size)"
      ],
      "metadata": {
        "id": "Ohy-Bc9UY5HE"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_steps = 100\n",
        "\n",
        "for step in range(max_steps):\n",
        "    t0 = time.time()\n",
        "    last_step = (step == max_steps )\n",
        "\n",
        "    # once in a while generate from the model (except step 0, which is noise)\n",
        "    if ((step > 0 and step % 250 == 0) or last_step) and (not use_compile):\n",
        "        model.eval()\n",
        "        num_return_sequences = 4\n",
        "        max_length = 32\n",
        "        tokens = enc.encode(\"سلام من شاهنامه هستم\") # interesting start for shahnameh model :))\n",
        "        tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "        tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "        xgen = tokens.to(device)\n",
        "        sample_rng = torch.Generator(device=device)\n",
        "        sample_rng.manual_seed(42)\n",
        "        while xgen.size(1) < max_length:\n",
        "            # forward the model to get the logits\n",
        "            with torch.no_grad():\n",
        "                with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "                    logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "                # take the logits at the last position\n",
        "                logits = logits[:, -1, :] # (B, vocab_size)\n",
        "                # get the probabilities\n",
        "                probs = F.softmax(logits, dim=-1)\n",
        "                # do top-k sampling of 50 (huggingface pipeline default)\n",
        "                # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "                topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "                # select a token from the top-k probabilities\n",
        "                # note: multinomial does not demand the input to sum to 1\n",
        "                ix = torch.multinomial(topk_probs, 1, generator=sample_rng) # (B, 1)\n",
        "                # gather the corresponding indices\n",
        "                xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "                # append to the sequence\n",
        "                xgen = torch.cat((xgen, xcol), dim=1)\n",
        "        # print the generated text\n",
        "        for i in range(num_return_sequences):\n",
        "            tokens = xgen[i, :max_length].tolist()\n",
        "            decoded = enc.decode(tokens)\n",
        "            print(f\"rank {ddp_rank} sample {i}: {decoded}\")\n",
        "\n",
        "    # do one step of the optimization\n",
        "    model.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_accum = 0.0\n",
        "    for micro_step in range(grad_accum_steps):\n",
        "        # added after video, this field is also used by the forward pass.\n",
        "        batch_idx, (X_batch, y_batch) = next(enumerate(train_loader))\n",
        "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(X_batch, y_batch)\n",
        "        # we have to scale the loss to account for gradient accumulation,\n",
        "        # because the gradients just add on each successive backward().\n",
        "        # addition of gradients corresponds to a SUM in the objective, but\n",
        "        # instead of a SUM we want MEAN. Scale the loss here so it comes out right\n",
        "        loss = loss / grad_accum_steps\n",
        "        loss_accum += loss.detach()\n",
        "        loss.backward()\n",
        "    norm = torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "    # determine and set the learning rate for this iteration\n",
        "    lr = get_lr(step)\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr\n",
        "    optimizer.step()\n",
        "    if device_type == \"cuda\":\n",
        "        torch.cuda.synchronize() # wait for the GPU to finish work\n",
        "    t1 = time.time()\n",
        "    dt = t1 - t0 # time difference in seconds\n",
        "    batch_size = 64\n",
        "    # tokens_processed = train_loader.B * train_loader.T * grad_accum_steps * ddp_world_size\n",
        "    tokens_processed = batch_size * seq_length * grad_accum_steps * ddp_world_size\n",
        "    tokens_per_sec = tokens_processed / dt\n",
        "    if loss_accum.item() < 0.0078:\n",
        "        print(\"we reached desirable loss === \", loss_accum.item())\n",
        "    if master_process:\n",
        "        print(f\"step {step:5d} | loss: {loss_accum.item():.6f} | lr {lr:.4e} | norm: {norm:.4f} | dt: {dt*1000:.2f}ms | tok/sec: {tokens_per_sec:.2f}\")\n",
        "        with open(log_file, \"a\") as f:\n",
        "            f.write(f\"{step} train {loss_accum.item():.6f}\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIQ6ds9EY5Jn",
        "outputId": "de8c5591-be96-43e1-c169-d2400f9fbbe7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step     0 | loss: 11.046612 | lr 8.3916e-07 | norm: 41.0961 | dt: 15799.79ms | tok/sec: 33183.23\n",
            "step     1 | loss: 10.901855 | lr 1.6783e-06 | norm: 40.7992 | dt: 14052.13ms | tok/sec: 37310.21\n",
            "step     2 | loss: 10.643498 | lr 2.5175e-06 | norm: 39.4163 | dt: 14313.78ms | tok/sec: 36628.19\n",
            "step     3 | loss: 10.286939 | lr 3.3566e-06 | norm: 34.1556 | dt: 14604.46ms | tok/sec: 35899.16\n",
            "step     4 | loss: 9.904593 | lr 4.1958e-06 | norm: 27.5740 | dt: 14885.33ms | tok/sec: 35221.79\n",
            "step     5 | loss: 9.533874 | lr 5.0350e-06 | norm: 22.4075 | dt: 15111.78ms | tok/sec: 34693.99\n",
            "step     6 | loss: 9.230017 | lr 5.8741e-06 | norm: 18.5954 | dt: 15410.68ms | tok/sec: 34021.08\n",
            "step     7 | loss: 8.952619 | lr 6.7133e-06 | norm: 16.0559 | dt: 15802.56ms | tok/sec: 33177.40\n",
            "step     8 | loss: 8.688454 | lr 7.5524e-06 | norm: 13.9756 | dt: 15610.62ms | tok/sec: 33585.35\n",
            "step     9 | loss: 8.469163 | lr 8.3916e-06 | norm: 12.2618 | dt: 15496.85ms | tok/sec: 33831.90\n",
            "step    10 | loss: 8.280221 | lr 9.2308e-06 | norm: 11.0359 | dt: 15372.04ms | tok/sec: 34106.61\n",
            "step    11 | loss: 8.081273 | lr 1.0070e-05 | norm: 10.2520 | dt: 15554.20ms | tok/sec: 33707.17\n",
            "step    12 | loss: 7.911824 | lr 1.0909e-05 | norm: 9.6140 | dt: 15573.74ms | tok/sec: 33664.88\n",
            "step    13 | loss: 7.756236 | lr 1.1748e-05 | norm: 9.1773 | dt: 15452.29ms | tok/sec: 33929.48\n",
            "step    14 | loss: 7.575249 | lr 1.2587e-05 | norm: 9.3135 | dt: 15501.95ms | tok/sec: 33820.79\n",
            "step    15 | loss: 7.426303 | lr 1.3427e-05 | norm: 9.7592 | dt: 15502.89ms | tok/sec: 33818.73\n",
            "step    16 | loss: 7.247164 | lr 1.4266e-05 | norm: 10.1628 | dt: 15651.29ms | tok/sec: 33498.07\n",
            "step    17 | loss: 7.067664 | lr 1.5105e-05 | norm: 10.6146 | dt: 15584.32ms | tok/sec: 33642.01\n",
            "step    18 | loss: 6.860715 | lr 1.5944e-05 | norm: 11.1883 | dt: 15531.46ms | tok/sec: 33756.52\n",
            "step    19 | loss: 6.695206 | lr 1.6783e-05 | norm: 9.4058 | dt: 15506.34ms | tok/sec: 33811.20\n",
            "step    20 | loss: 6.482511 | lr 1.7622e-05 | norm: 8.5052 | dt: 15568.22ms | tok/sec: 33676.81\n",
            "step    21 | loss: 6.336264 | lr 1.8462e-05 | norm: 7.6868 | dt: 15661.80ms | tok/sec: 33475.59\n",
            "step    22 | loss: 6.122301 | lr 1.9301e-05 | norm: 6.7958 | dt: 15542.44ms | tok/sec: 33732.68\n",
            "step    23 | loss: 5.966477 | lr 2.0140e-05 | norm: 6.2256 | dt: 15555.01ms | tok/sec: 33705.40\n",
            "step    24 | loss: 5.794145 | lr 2.0979e-05 | norm: 6.0047 | dt: 15563.41ms | tok/sec: 33687.21\n",
            "step    25 | loss: 5.664368 | lr 2.1818e-05 | norm: 5.6582 | dt: 15587.38ms | tok/sec: 33635.43\n",
            "step    26 | loss: 5.579033 | lr 2.2657e-05 | norm: 5.5447 | dt: 15679.16ms | tok/sec: 33438.53\n",
            "step    27 | loss: 5.477455 | lr 2.3497e-05 | norm: 5.1442 | dt: 15573.79ms | tok/sec: 33664.77\n",
            "step    28 | loss: 5.361848 | lr 2.4336e-05 | norm: 4.8956 | dt: 15574.92ms | tok/sec: 33662.32\n",
            "step    29 | loss: 5.274127 | lr 2.5175e-05 | norm: 5.0194 | dt: 15583.42ms | tok/sec: 33643.97\n",
            "step    30 | loss: 5.189188 | lr 2.6014e-05 | norm: 4.8973 | dt: 15594.12ms | tok/sec: 33620.88\n",
            "step    31 | loss: 5.113332 | lr 2.6853e-05 | norm: 4.8296 | dt: 15678.63ms | tok/sec: 33439.66\n",
            "step    32 | loss: 5.043097 | lr 2.7692e-05 | norm: 4.7346 | dt: 15596.78ms | tok/sec: 33615.13\n",
            "step    33 | loss: 4.992220 | lr 2.8531e-05 | norm: 4.6642 | dt: 15596.22ms | tok/sec: 33616.34\n",
            "step    34 | loss: 4.950408 | lr 2.9371e-05 | norm: 4.3925 | dt: 15570.00ms | tok/sec: 33672.96\n",
            "step    35 | loss: 4.857373 | lr 3.0210e-05 | norm: 4.6902 | dt: 15574.35ms | tok/sec: 33663.56\n",
            "step    36 | loss: 4.837359 | lr 3.1049e-05 | norm: 4.4668 | dt: 15672.43ms | tok/sec: 33452.89\n",
            "step    37 | loss: 4.757659 | lr 3.1888e-05 | norm: 4.5444 | dt: 15570.62ms | tok/sec: 33671.61\n",
            "step    38 | loss: 4.762235 | lr 3.2727e-05 | norm: 4.8024 | dt: 15546.53ms | tok/sec: 33723.80\n",
            "step    39 | loss: 4.744568 | lr 3.3566e-05 | norm: 4.3093 | dt: 15547.12ms | tok/sec: 33722.53\n",
            "step    40 | loss: 4.610276 | lr 3.4406e-05 | norm: 4.2919 | dt: 15569.73ms | tok/sec: 33673.55\n",
            "step    41 | loss: 4.625603 | lr 3.5245e-05 | norm: 4.5712 | dt: 15651.79ms | tok/sec: 33497.01\n",
            "step    42 | loss: 4.583002 | lr 3.6084e-05 | norm: 3.9103 | dt: 15497.91ms | tok/sec: 33829.60\n",
            "step    43 | loss: 4.545671 | lr 3.6923e-05 | norm: 4.1644 | dt: 15533.37ms | tok/sec: 33752.36\n",
            "step    44 | loss: 4.485522 | lr 3.7762e-05 | norm: 4.2243 | dt: 15514.09ms | tok/sec: 33794.31\n",
            "step    45 | loss: 4.471016 | lr 3.8601e-05 | norm: 3.9751 | dt: 15527.89ms | tok/sec: 33764.28\n",
            "step    46 | loss: 4.422350 | lr 3.9441e-05 | norm: 4.0255 | dt: 15692.23ms | tok/sec: 33410.67\n",
            "step    47 | loss: 4.378023 | lr 4.0280e-05 | norm: 4.0482 | dt: 15546.80ms | tok/sec: 33723.20\n",
            "step    48 | loss: 4.349328 | lr 4.1119e-05 | norm: 3.9510 | dt: 15521.14ms | tok/sec: 33778.96\n",
            "step    49 | loss: 4.284941 | lr 4.1958e-05 | norm: 3.7545 | dt: 15511.01ms | tok/sec: 33801.03\n",
            "step    50 | loss: 4.293005 | lr 4.2797e-05 | norm: 3.9130 | dt: 15513.34ms | tok/sec: 33795.95\n",
            "step    51 | loss: 4.217795 | lr 4.3636e-05 | norm: 3.9577 | dt: 15650.08ms | tok/sec: 33500.65\n",
            "step    52 | loss: 4.172632 | lr 4.4476e-05 | norm: 3.9231 | dt: 15504.88ms | tok/sec: 33814.38\n",
            "step    53 | loss: 4.130248 | lr 4.5315e-05 | norm: 3.8986 | dt: 15569.74ms | tok/sec: 33673.52\n",
            "step    54 | loss: 4.080301 | lr 4.6154e-05 | norm: 3.9031 | dt: 15511.01ms | tok/sec: 33801.03\n",
            "step    55 | loss: 4.058393 | lr 4.6993e-05 | norm: 3.8522 | dt: 15518.47ms | tok/sec: 33784.76\n",
            "step    56 | loss: 4.023993 | lr 4.7832e-05 | norm: 4.0495 | dt: 15624.56ms | tok/sec: 33555.37\n",
            "step    57 | loss: 3.976432 | lr 4.8671e-05 | norm: 3.7028 | dt: 15486.44ms | tok/sec: 33854.66\n",
            "step    58 | loss: 3.914542 | lr 4.9510e-05 | norm: 3.6008 | dt: 15499.51ms | tok/sec: 33826.10\n",
            "step    59 | loss: 3.881095 | lr 5.0350e-05 | norm: 3.6786 | dt: 15496.40ms | tok/sec: 33832.89\n",
            "step    60 | loss: 3.841028 | lr 5.1189e-05 | norm: 3.5163 | dt: 15475.04ms | tok/sec: 33879.58\n",
            "step    61 | loss: 3.819454 | lr 5.2028e-05 | norm: 3.6632 | dt: 15619.63ms | tok/sec: 33565.96\n",
            "step    62 | loss: 3.767042 | lr 5.2867e-05 | norm: 3.7351 | dt: 15476.86ms | tok/sec: 33875.59\n",
            "step    63 | loss: 3.704781 | lr 5.3706e-05 | norm: 3.4627 | dt: 15469.78ms | tok/sec: 33891.11\n",
            "step    64 | loss: 3.693053 | lr 5.4545e-05 | norm: 3.6330 | dt: 15450.32ms | tok/sec: 33933.80\n",
            "step    65 | loss: 3.616017 | lr 5.5385e-05 | norm: 3.6206 | dt: 15436.92ms | tok/sec: 33963.25\n",
            "step    66 | loss: 3.572093 | lr 5.6224e-05 | norm: 3.3895 | dt: 15547.02ms | tok/sec: 33722.72\n",
            "step    67 | loss: 3.534510 | lr 5.7063e-05 | norm: 3.4657 | dt: 15448.97ms | tok/sec: 33936.76\n",
            "step    68 | loss: 3.485411 | lr 5.7902e-05 | norm: 3.3096 | dt: 15463.34ms | tok/sec: 33905.22\n",
            "step    69 | loss: 3.469860 | lr 5.8741e-05 | norm: 3.5375 | dt: 15471.62ms | tok/sec: 33887.08\n",
            "step    70 | loss: 3.380333 | lr 5.9580e-05 | norm: 3.2258 | dt: 15476.93ms | tok/sec: 33875.45\n",
            "step    71 | loss: 3.340122 | lr 6.0420e-05 | norm: 3.1909 | dt: 15603.12ms | tok/sec: 33601.49\n",
            "step    72 | loss: 3.324567 | lr 6.1259e-05 | norm: 3.2565 | dt: 15566.56ms | tok/sec: 33680.41\n",
            "step    73 | loss: 3.269231 | lr 6.2098e-05 | norm: 3.5143 | dt: 15479.79ms | tok/sec: 33869.18\n",
            "step    74 | loss: 3.222236 | lr 6.2937e-05 | norm: 3.0955 | dt: 15523.04ms | tok/sec: 33774.82\n",
            "step    75 | loss: 3.166275 | lr 6.3776e-05 | norm: 3.0453 | dt: 15488.09ms | tok/sec: 33851.03\n",
            "step    76 | loss: 3.143498 | lr 6.4615e-05 | norm: 3.0579 | dt: 15658.19ms | tok/sec: 33483.31\n",
            "step    77 | loss: 3.097248 | lr 6.5455e-05 | norm: 2.9792 | dt: 15564.42ms | tok/sec: 33685.03\n",
            "step    78 | loss: 3.041561 | lr 6.6294e-05 | norm: 2.8550 | dt: 15390.63ms | tok/sec: 34065.40\n",
            "step    79 | loss: 2.987419 | lr 6.7133e-05 | norm: 2.7743 | dt: 15395.72ms | tok/sec: 34054.14\n",
            "step    80 | loss: 2.948703 | lr 6.7972e-05 | norm: 2.8519 | dt: 15426.20ms | tok/sec: 33986.86\n",
            "step    81 | loss: 2.904122 | lr 6.8811e-05 | norm: 2.5170 | dt: 15484.03ms | tok/sec: 33859.91\n",
            "step    82 | loss: 2.848069 | lr 6.9650e-05 | norm: 2.5571 | dt: 15689.79ms | tok/sec: 33415.86\n",
            "step    83 | loss: 2.838237 | lr 7.0490e-05 | norm: 2.6297 | dt: 15516.32ms | tok/sec: 33789.45\n",
            "step    84 | loss: 2.779867 | lr 7.1329e-05 | norm: 2.4329 | dt: 15524.90ms | tok/sec: 33770.77\n",
            "step    85 | loss: 2.740825 | lr 7.2168e-05 | norm: 2.3140 | dt: 15497.09ms | tok/sec: 33831.38\n",
            "step    86 | loss: 2.701419 | lr 7.3007e-05 | norm: 2.4007 | dt: 15499.15ms | tok/sec: 33826.90\n",
            "step    87 | loss: 2.663900 | lr 7.3846e-05 | norm: 2.1105 | dt: 15648.24ms | tok/sec: 33504.59\n",
            "step    88 | loss: 2.628404 | lr 7.4685e-05 | norm: 2.1820 | dt: 15506.86ms | tok/sec: 33810.07\n",
            "step    89 | loss: 2.584446 | lr 7.5524e-05 | norm: 2.0962 | dt: 15535.25ms | tok/sec: 33748.28\n",
            "step    90 | loss: 2.554059 | lr 7.6364e-05 | norm: 1.9681 | dt: 15550.84ms | tok/sec: 33714.46\n",
            "step    91 | loss: 2.518238 | lr 7.7203e-05 | norm: 1.7991 | dt: 15544.49ms | tok/sec: 33728.23\n",
            "step    92 | loss: 2.480880 | lr 7.8042e-05 | norm: 2.0260 | dt: 15745.18ms | tok/sec: 33298.32\n",
            "step    93 | loss: 2.462843 | lr 7.8881e-05 | norm: 1.9672 | dt: 15507.06ms | tok/sec: 33809.63\n",
            "step    94 | loss: 2.444023 | lr 7.9720e-05 | norm: 1.7171 | dt: 15504.23ms | tok/sec: 33815.80\n",
            "step    95 | loss: 2.409244 | lr 8.0559e-05 | norm: 1.5403 | dt: 15508.78ms | tok/sec: 33805.89\n",
            "step    96 | loss: 2.384042 | lr 8.1399e-05 | norm: 1.5714 | dt: 15636.00ms | tok/sec: 33530.82\n",
            "step    97 | loss: 2.350441 | lr 8.2238e-05 | norm: 1.6512 | dt: 15605.46ms | tok/sec: 33596.45\n",
            "step    98 | loss: 2.315632 | lr 8.3077e-05 | norm: 1.3477 | dt: 15433.49ms | tok/sec: 33970.81\n",
            "step    99 | loss: 2.299102 | lr 8.3916e-05 | norm: 1.4237 | dt: 15392.81ms | tok/sec: 34060.57\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokens = enc.encode(\"از من چه بر آید:\")\n",
        "tokens = torch.tensor(tokens, dtype=torch.long)\n",
        "num_return_sequences = 5\n",
        "\n",
        "tokens = tokens.unsqueeze(0).repeat(num_return_sequences, 1)\n",
        "\n",
        "\n",
        "max_length = 40\n",
        "xgen = tokens.to(device)\n",
        "while xgen.size(1) < max_length:\n",
        "    # forward the model to get the logits\n",
        "    with torch.no_grad():\n",
        "        with torch.autocast(device_type=device_type, dtype=torch.bfloat16):\n",
        "            logits, loss = model(xgen) # (B, T, vocab_size)\n",
        "        # take the logits at the last position\n",
        "        last_logits = logits[:, -1, :] # (B, vocab_size)\n",
        "        # get the probabilities\n",
        "        probs = F.softmax(last_logits, dim=-1)\n",
        "        # do top-k sampling of 50 (huggingface pipeline default)\n",
        "        # topk_probs here becomes (5, 50), topk_indices is (5, 50)\n",
        "        topk_probs, topk_indices = torch.topk(probs, 50, dim=-1)\n",
        "        # select a token from the top-k probabilities\n",
        "        # note: multinomial does not demand the input to sum to 1\n",
        "        ix = torch.multinomial(topk_probs, 1) # (B, 1)\n",
        "        # gather the corresponding indices\n",
        "        xcol = torch.gather(topk_indices, -1, ix) # (B, 1)\n",
        "        # append to the sequence\n",
        "        xgen = torch.cat((xgen, xcol), dim=1)\n",
        "# print the generated text\n",
        "for i in range(num_return_sequences):\n",
        "    tokenss = xgen[i, :max_length].tolist()\n",
        "    decoded = enc.decode(tokenss)\n",
        "    print(f\"rank {ddp_rank} sample {i}: {decoded}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEiVDlc3Y5L9",
        "outputId": "f13b7fec-8205-4c53-ad59-7785fa36a343"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rank 0 sample 0: از من چه بر آید: با\n",
            "|چین\n",
            "|بدهد توده ا\n",
            "rank 0 sample 1: از من چه بر آید:\n",
            "|به برپای ناز ب\n",
            "|ه \n",
            "rank 0 sample 2: از من چه بر آید:پشیان پهمی بخ به آ\n",
            "rank 0 sample 3: از من چه بر آید:یوا رار بایشمر نراهر\n",
            "rank 0 sample 4: از من چه بر آید:پر ند\n",
            "|بد\n",
            "|بدل بهرش ا\n"
          ]
        }
      ]
    }
  ]
}